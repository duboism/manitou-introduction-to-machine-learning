\section[A simple example: MLP]{A simple example: classify handwritten digits with a \acl{MLP}}

\begin{frame}
  \note{
    \begin{itemize}
      \item Preprocessing
      \item Supervised learning
    \end{itemize}
  }

  \frametitle{The MNIST dataset}

  \begin{columns}
    \begin{column}{0.5\textwidth}
      \begin{itemize}
      \item Classic dataset
        \begin{itemize}
        \item Lots of methods have been tested with it
        \end{itemize}
      \item<2-> All images have the same size ($28 \times 28$) and aspect ratio
        \begin{itemize}
        \item Non-trivial pre-processing
        \end{itemize}
      \item<2-> Train set: 60 000 images
        \begin{itemize}
        \item $\trainingSet =
          \setpredicate{\couple{\example_{i}}{\knownLabel_{i}}}{i=1..\nTrainingSamples}$
        \item $\example_i \in \setR^{\dimExample}, \dimExample = 784$
        \item $\knownLabel_i \in \intinterval{0}{9}$
        \end{itemize}
      \item<2-> Test set: 10 000 images
      \item<3-> The task is to recognize the digit on the image
        % \ie{} we are looking
        % for a function: $\decisionFunc: \, \setR^{\dimExample} \mapsto \intinterval{0}{9}$
      \end{itemize}
    \end{column}
    \begin{column}{0.5\textwidth}
      \begin{tikzpicture}[>=stealth]
        \node[anchor=south west,inner sep=0] (image) at (0,0) {
          \includegraphics[width=\columnwidth]{img/MnistExamples.png}
        };
        \onslide<3->{
        \begin{scope}[x={(image.south east)},y={(image.north west)}]
          \draw[red,ultra thick] (0.22,0.15) rectangle (0.28,0.25);
        \end{scope}
        }
      \end{tikzpicture}
      \url{http://yann.lecun.com/exdb/mnist/index.html}
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}
  \frametitle{\acl{MLP}: Introduction}

  \begin{textblock}{80}(5, 20)
    \begin{itemize}
    \item<1-> First perceptrons without hidden layers and activation functions
      (introduced in 1957) were
      very limited
      \begin{itemize}
      \item Many improvements since then
      \item Lots of variations
      \end{itemize}
    \item<1-> Loose biological inspiration
    \end{itemize}
  \end{textblock}
\end{frame}

\begin{frame}
  \note{
    \begin{itemize}
    \item Probabilistic classifier:
      \begin{align*}
        \begin{aligned}
          & \decisionFunc_{\params} : & \setR^{\dimExample} & \rightarrow & \interval{0}{1}^{\nClasses} \\
        \end{aligned}
      \end{align*}
      where
      $\vector{\predLabel} = \apply{\decisionFunc_{\params}}{\example}$ is a vector such
      that $\vectorElem{\vector{\predLabel}}{k}$ is $\prob{\example = k}$
    \item Feed-forward
    \item Parametrized function
    \item Importance of non-linear activation function
    \end{itemize}
  }

  \frametitle{\acl{MLP}: Computation}
  \begin{textblock}{100}(5,20)
    \begin{tikzpicture}[>=stealth]
      % Inspired by https://tex.stackexchange.com/a/153974
      % We display a MLP with 2 hidden layers for OCR
      % The input image and the output layer are always displayed
      % The layers (input and the 2 hidden) and their connections appear after
      % Description of layers are on the graph

      % Parameters
      \def\xInputImage{0}
      \def\nameInputImage{input-name}
      \def\xInput{2.0}
      \def\nameInput{input}
      \def\xHiddenI{4.3}
      \def\nameHiddenI{hidden1}
      \def\xHiddenII{6.6}
      \def\nameHiddenII{hidden2}
      \def\xOutput{8.9}
      \def\nameOutput{output}
      \def\xProba{9.4}
      \def\nameProba{proba}
      \def\missing{missing}

      % Input image and grid
      % This part is inspired by https://tex.stackexchange.com/a/128648
      % and a lot of trial and error for the grid (including a bit of ChatGPT)
      % Apparently, the idea is to fix the size of the image and then the grid
      % flows
      % I use 56px (twice the real size) so that it looks OK and computations are easy.
      \node[anchor=center,inner sep=0pt,draw=black] (\nameInputImage) at (\xInputImage,0) {
        \includegraphics[width=56px,height=56px]{img/Mnist_8.png}
      };
      \begin{scope}
        \clip (\nameInputImage.south west) rectangle (\nameInputImage.north east);
        \draw[step=2px,gray,very thin] (\nameInputImage.south west) grid (56px, 56px);
      \end{scope}

      \onslide<2->{
      % Input layer
      \node [align=center, above] at (\xInput,2.2) {%Input \\ layer \\
          {\tiny $28\times28=784$ units}};
      \foreach \m/\l [count=\y] in {1,2,3,missing,784}
      \node [every neuron/.try, neuron \m/.try] (\nameInput-\m) at (\xInput,3.0-\y) {};

      % Connections from input image to input layer
      % No loop here cause we need to shift the positions
      % It uses the scale introduced above
      \draw [->] (\nameInputImage)++(-27px,27px) -- (\nameInput-1);
      \draw [->] (\nameInputImage)++(-25px,27px) -- (\nameInput-2);
      \draw [->] (\nameInputImage)++(-23px,27px) -- (\nameInput-3);
      \draw [->] (\nameInputImage)++(27px,-27px) -- (\nameInput-784);
      }

      % Description of the output
      \onslide<2>{
      \node[text width=3cm,anchor=north west] at (\xInput-1, -2.2) {{\footnotesize
          $\output^{1}_{k} = \vectorElem{\example}{k}$
      }} ;
      }

      % Matrix form
      \onslide<3->{
      \node[text width=3cm,anchor=north west] at (\xInput-1, -2.2) {{\footnotesize
          $\vector{\output}^{1} = \example$
      }} ;
      }

      % First hidden layer
      % Connections are displayed in 2 different slides (see below)
      \onslide<3->{
      \node [align=center, above] at (\xHiddenI,2.2) {%Hidden \\ layer \\
          {\tiny 200 $\ReLUFunc$ units}};
      \foreach \m [count=\y] in {1,2,missing,3}
      \node [every neuron/.try, neuron \m/.try ] (\nameHiddenI-\m) at (\xHiddenI,2.5-\y) {};
      }

      % Connections from input layer to first neuron with weights
      % and explanation of the output
      \onslide<3>{
      \foreach \p in {1,2,3,784}
      \foreach \k in {1}
      \draw[->] (\nameInput-\p) -- node {${\color{red} \weight^{1}_{\k,\p}}$} (\nameHiddenI-\k);

      % Description of the output
      \node[text width=3cm,anchor=north west] at (\xHiddenI-1, -2.2) {{\footnotesize
          $s^{2}_{k} = \sum_{p=1}^{N_{1}} {\color{red} \weight^{1}_{k,p}} \output^{1}_{p} $
      }};
      \node[text width=5cm,anchor=north west] at (\xHiddenI-1, -3.0) {{\footnotesize
          $\output^{2}_{k} = \apply{\ReLUFuncExt}{0, s^{2}_{k}}$ ($\ReLUFunc$)
      }};
      }

      % Other connections from input layer to first hidden layer
      % and matrix form
      \onslide<4->{
      \foreach \i in {1,2,3,784}
      \foreach \j in {1,...,3}
      \draw[->] (\nameInput-\i) -- (\nameHiddenI-\j);
      \node[text width=3cm,anchor=north west] at (\xHiddenI-1.2, -2.2) {{\footnotesize
          $\vector{\output^{2}} = \apply{\ReLUFunc}{{\color{red}\weightMatrix^{1}}\vector{\output^{1}}}$
      }};
      }

      % Second hidden layer
      \onslide<5->{
      \node [align=center, above] at (\xHiddenII,2.2) {% Hidden \\ layer \\
          {\tiny 150 $\ReLUFunc$ units}};
      \foreach \m [count=\y] in {1,2,missing,3}
      \node [every neuron/.try, neuron \m/.try ] (\nameHiddenII-\m) at (\xHiddenII,2.2-0.9*\y) {};

      % Connections to first hidden layer to second hidden layer
      \foreach \i in {1,2,3}
      \foreach \j in {1,2,3}
      \draw [->] (\nameHiddenI-\i) -- (\nameHiddenII-\j);

      \node[text width=3cm,anchor=north west] at (\xHiddenII-1, -2.4) {{\footnotesize
          $\vector{\output^{3}} = \apply{\ReLUFunc}{{\color{red}\weightMatrix^{2}}\vector{\output^{2}}}$
      }};
      }

      % Output layer
      \node [align=center, above] at (\xOutput,2.2) {% Output \\ layer \\
        {\tiny 10 $\softMaxFunc$ units}};
      \foreach \m [count=\y] in {0,1,missing,9}
      \node [every neuron/.try, neuron \m/.try ] (\nameOutput-\m) at (\xOutput,1.6-0.7*\y) {};

      \onslide<6->{
      % Connection from second hidden layer to output layer
      \foreach \i in {1,2,3}
      \foreach \j in {0,1,9}
      \draw [->] (\nameHiddenII-\i) -- (\nameOutput-\j);
      }

      % Description of output
      \onslide<6>{
      \node[text width=3cm,anchor=north west] at (\xOutput-0.5, -2.2) {{\footnotesize
          $s^{4}_{k} = \sum_{p=1}^{N_{3}} {\color{red} \weight^{3}_{k,p}} \output^{3}_{p}$
      }};
      \node[text width=5cm,anchor=north west] at (\xOutput-0.5, -3.0) {{\footnotesize
          $\output^{4}_{k} = \frac{\apply{\exp}{s^{4}_{k}}}{\sum_{j=1}^{10} \apply{\exp}{s^{4}_{j}}}$
      }};
      }

      \onslide<7->{
      \node[text width=3cm,anchor=north west] at (\xOutput-0.7, -2.2) {{\footnotesize
          $\vector{\output^{4}} = \apply{\softMaxFunc}{{\color{red}\weightMatrix^{3}}\vector{\output^{3}}}$
      }};
      }


      % Output probabilities
      \foreach \m [count=\y] in {0,1,missing,9}
      \node (\nameProba-\m) at (\xProba,1.6-0.7*\y) {\ifx\m\missing\else {\tiny $\prob{\m}$}\fi};

      \onslide<7->{
      \node[text width=10cm,anchor=north west] at (-1, -3) {{\footnotesize
          \acsu{MLP} is a parametrized function $\decisionFunc_{\params}$ where
          ${\color{red}\params}$ = vector containing the {\tiny $784 \times
          200 + 200 \times 150 + 150 \times 10 = $} 188 300 weights
      }};
      }
    \end{tikzpicture}

  \end{textblock}

\end{frame}


\begin{frame}
  \frametitle{\acl{MLP}: Learning (1/2)}

  \begin{textblock}{90}(5,15)
    \begin{itemize}
    \item Adjust $\params$ to give a better performance
    \item<2-> Loss function for an example:
      $\apply{\lossFunc}{%
        \apply{\decisionFunc_{\params}}{%
          \example_i
        },
        \knownLabel_i
      }$
    \item<2-> Find:
      \[
        \estim{\params} = \argmin_{\params}{
          \frac{1}{\nTrainingSamples} \sum_{i=1}^{\nTrainingSamples}
          \apply{\lossFunc}{
            \apply{\decisionFunc_\params}{\example_i},
            \knownLabel_i
          }
        }
      \]
    \end{itemize}
  \end{textblock}
\end{frame}



\begin{frame}
  \frametitle{\acl{MLP}: Learning (2/2)}

  \begin{textblock}{45}(5,15)
    \begin{itemize}
    \item How to minimize $\lossFunc$?
    \item<2-> Gradient descent:
      \begin{itemize}
      \item Gradient of $\lossFunc$ at $\params_{t}$: $\apply{\grad_{\params}\lossFunc}{\params_{t}}$
      \item<3-> $\params_{t+1} \leftarrow \params_{t} - \learningRate \apply{\grad_{\params}\lossFunc}{\params_{t}}$
      \end{itemize}
    \end{itemize}
  \end{textblock}
  \begin{textblock}{50}(50,30)
    \begin{center}
    \begin{tikzpicture}[domain=0:4]
      % Display a 1-D analytical loss function (x-2)**2 + 0.3
      % to illustrate
      \def\paramsInit{3} % Current value of the parameter
      \def\lossInit{1.3} % Current value of the loss
      \def\gradVal{2}    % Value of the gradient

      % \draw[very thin,color=gray] (-0.1,-0.1) grid (3.9,3.9);
      \draw[->] (-0.2,0) -- (4.2,0) node[right] {$\params$};
      \draw[->] (0,-0.2) -- (0,4.2) node[above right] {$\apply{\lossFunc}{\apply{\decisionFunc_{\params}}{\example_i}, \knownLabel_i}$};
      % Display loss function
      \draw[color=blue] plot function{(x-2)**2+0.3} node[right] {};

      % Value at \paramsInit
      \node[anchor=north] at (\paramsInit, 0) {$\params_{t}$};
      \draw[dashed] (\paramsInit, 0) -- (\paramsInit, \lossInit) {};

      % Gradient at \paramsInit
      \onslide<2->{
      \draw[->,color=red,thick] (\paramsInit, \lossInit) -- (\paramsInit + 0.5, \lossInit + 0.5*\gradVal) {};
      \node[anchor=west] at (\paramsInit + 0.5, \lossInit + 0.5*\gradVal) {${\color{red}\apply{\grad_{\params}\lossFunc}{\params_{t}}}$};
      }
    \end{tikzpicture}
    \end{center}
  \end{textblock}
\end{frame}


\begin{frame}
  \frametitle{\acl{MLP}: Back-propagation (1/2)}
  \begin{itemize}
    \item Gradient computation in large dimension:

    $
    \grad_{\params} \apply{\lossFunc}{\apply{\decisionFunc_{\params}}{\example_i}, \knownLabel_i} =
    \left( \frac{\partial \apply{\lossFunc}{\apply{\decisionFunc_{\params}}{\example_i}, \knownLabel_i} }{\partial w_{1,1}^1},\ldots,\frac{\partial \apply{\lossFunc}{\apply{\decisionFunc_{\params}}{\example_i}, \knownLabel_i} }{\partial w_{k,p}^l},\ldots,\frac{\partial \apply{\lossFunc}{\apply{\decisionFunc_{\params}}{\example_i}, \knownLabel_i} }{\partial w_{N_{4},N_{3}}^{3}} \right)^T
    $

    %$
    %\grad_{\params} \frac{1}{m} \sum_{i=1}^m \apply{\lossFunc}{\apply{\decisionFunc_{\params}}{\example_i}, \knownLabel_i} =
    %\frac{1}{m} \sum_{i=1}^m \left( \frac{\partial \apply{\lossFunc}{\apply{\decisionFunc_{\params}}{\example_i}, \knownLabel_i} }{\partial w_{1,1}^1},\ldots,\frac{\partial \apply{\lossFunc}{\apply{\decisionFunc_{\params}}{\example_i}, \knownLabel_i} }{\partial w_{k,p}^l},\ldots,\frac{\partial \apply{\lossFunc}{\apply{\decisionFunc_{\params}}{\example_i}, \knownLabel_i} }{\partial w_{N_{L-1},N_{L}}^{N_{L-1}}} \right)^T
    %$
    \begin{itemize}
    \item Analytical calculation is possible but tedious
    \end{itemize}
  \item $\decisionFunc_{\params}$ is a
      composition of functions:
      \[
        \apply{\decisionFunc_{\params}}{\example} = \apply{\softMaxFunc}{%
          \weightMatrix^3 \times
          \apply{\ReLUFunc}{%
            \weightMatrix^2 \times
            \apply{\ReLUFunc}{%
              \weightMatrix^1 \times \example
            }
          }
        }
      \]
      \begin{itemize}
      \item Uses the chain rule to compute partial derivatives.
      \end{itemize}
    %$
    %\frac{\partial \apply{\lossFunc}{\apply{\decisionFunc_{\params}}{\example_i}, \knownLabel_i} }{\partial w_{k,p}^l} =
    %\frac{\partial \apply{\lossFunc}{\apply{\decisionFunc_{\params}}{\example_i}, \knownLabel_i}}{\partial o_k^{l+1}} \frac{\partial o_k^{l+1}}{\partial s_k^{l+1}} \frac{\partial s_k^{l+1}}{\partial w_{k,p}^{l}}
    %$
  \end{itemize}

\end{frame}

\begin{frame}
  \frametitle{\acl{MLP}: Back-propagation (2/2)}
  \begin{block}{Derivation of the gradient}
  \begin{itemize}
    \item Apply the chain rule at layer $\ell$:

    $
    \frac{\partial \apply{\lossFunc}{\apply{\decisionFunc_{\params}}{\example_i}, \knownLabel_i} }{\partial w_{k,p}^\ell} =
    \frac{\partial \apply{\lossFunc}{\apply{\decisionFunc_{\params}}{\example_i}, \knownLabel_i}}{\partial o_k^{\ell+1}} \frac{\partial o_k^{\ell+1}}{\partial s_k^{\ell+1}} \frac{\partial s_k^{\ell+1}}{\partial w_{k,p}^{\ell}}
    $

    \item $\frac{\partial \apply{\lossFunc}{\apply{\decisionFunc_{\params}}{\example_i}, \knownLabel_i}}{\partial o_k^{\ell+1}}$: known at the $\ell+1$ layer. $\frac{\partial \apply{\lossFunc}{\apply{\decisionFunc_{\params}}{\example_i}, \knownLabel_i}}{\partial o_k^{L}}$: last layer, $\lossFunc$ must be differentiable w.r.t $o_k^{L}$.

    %\item $\frac{\partial \apply{\lossFunc}{\apply{\decisionFunc_{\params}}{\example_i}, \knownLabel_i}}{\partial o_k^{L}}$: last layer, $\lossFunc$ must be differentiable w.r.t $o_k^{L}$

    \item $\frac{\partial o_k^{\ell+1}}{\partial s_k^{\ell+1}}$: $o_k^{\ell+1} = \activFunc(s_k^{\ell+1})$, $\activFunc$ as intermediate or final activation function must be differentiable.

    \item $\frac{\partial s_k^{\ell+1}}{\partial w_{k,p}^{\ell}} = \frac{\partial}{\partial w_{k,p}^{\ell}} \sum_{p' \in \left\{1,\ldots,N_l \right\}}  w_{p', k}^{\ell}o_{p'}^{k}= \frac{\partial }{\partial w_{k,p}^{\ell}}  w_{p, k}^{\ell}o_{p}^{\ell}=o_{p}^{\ell}$

    \end{itemize}
  \end{block}

  \begin{block}{General idea}<2->
    \begin{itemize}
    \item Back-propagation is a generic idea for feed-forward models
    \end{itemize}
  \end{block}
\end{frame}
