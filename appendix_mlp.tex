% Appendices
% To be used with hyper links

\begin{frame}[label=Backpropagation]
  \frametitle{\acl{MLP}: Compute the gradient (1/2)}

  \begin{textblock}{90}(5, 15)
    \begin{itemize}
    \item Gradient:
      $
      \grad_{\params} \apply{\lossFunc}{\apply{\hyp_{\params}}{\example_i}, \knownLabel_i} =
      \left( \partialDeriv{\apply{\lossFunc}{\apply{\hyp_{\params}}{\example_i},
            \knownLabel_i}}{w_{1,1}^1}, \ldots, \partialDeriv{\apply{\lossFunc}{\apply{\hyp_{\params}}{\example_i}, \knownLabel_i}}{w_{k,p}^l},\ldots,\partialDeriv{\apply{\lossFunc}{\apply{\hyp_{\params}}{\example_i}, \knownLabel_i}}{w_{N_{4},N_{3}}^{3}} \right)^T
      $

      % $
      % \grad_{\params} \partialDeriv{1}{m} \sum_{i=1}^m \apply{\lossFunc}{\apply{\hyp_{\params}}{\example_i}, \knownLabel_i} =
      % \partialDeriv{1}{m} \sum_{i=1}^m \left( \partialDeriv{\apply{\lossFunc}{\apply{\hyp_{\params}}{\example_i}, \knownLabel_i} }{w_{1,1}^1},\ldots,\partialDeriv{\apply{\lossFunc}{\apply{\hyp_{\params}}{\example_i}, \knownLabel_i} }{w_{k,p}^l},\ldots,\partialDeriv{\apply{\lossFunc}{\apply{\hyp_{\params}}{\example_i}, \knownLabel_i} }{w_{N_{L-1},N_{L}}^{N_{L-1}}} \right)^T
      % $
      \begin{itemize}
      \item Analytical calculation is possible but tedious
      \end{itemize}
    \item $\hyp_{\params}$ is a composition of functions:
      \[
        \apply{\hyp_{\params}}{\example} = \apply{\softMaxFunc}{%
          \weightMatrix^3 \times
          \apply{\ReLUFunc}{%
            \weightMatrix^2 \times
            \apply{\ReLUFunc}{%
              \weightMatrix^1 \times \example
            }
          }
        }
      \]
      \begin{itemize}
      \item Use the chain rule to compute partial derivatives.
      \end{itemize}
      % $
      % \partialDeriv{\apply{\lossFunc}{\apply{\hyp_{\params}}{\example_i}, \knownLabel_i}}{w_{k,p}^l} =
      % \partialDeriv{\apply{\lossFunc}{\apply{\hyp_{\params}}{\example_i}, \knownLabel_i}}{o_k^{l+1}} \partialDeriv{o_k^{l+1}}{s_k^{l+1}} \partialDeriv{s_k^{l+1}}{w_{k,p}^{l}}
      % $
    \end{itemize}
  \end{textblock}
\end{frame}

\begin{frame}
  \frametitle{\acl{MLP}: Compute the gradient (2/2)}

  \begin{textblock}{90}(5, 15)
    \begin{block}{Derivation of the gradient}
      \begin{itemize}
      \item Apply the chain rule at layer $\ell$:
        $
        \partialDeriv{\apply{\lossFunc}{\apply{\hyp_{\params}}{\example_i}, \knownLabel_i}}{w_{k,p}^\ell} =
        \partialDeriv{\apply{\lossFunc}{\apply{\hyp_{\params}}{\example_i}, \knownLabel_i}}{o_k^{\ell+1}} \partialDeriv{o_k^{\ell+1}}{s_k^{\ell+1}} \partialDeriv{s_k^{\ell+1}}{w_{k,p}^{\ell}}
        $

      \item $\partialDeriv{\apply{\lossFunc}{\apply{\hyp_{\params}}{\example_i}, \knownLabel_i}}{o_k^{\ell+1}}$: known at the $\ell+1$ layer. $\partialDeriv{\apply{\lossFunc}{\apply{\hyp_{\params}}{\example_i}, \knownLabel_i}}{o_k^{L}}$: last layer, $\lossFunc$ must be differentiable w.r.t $o_k^{L}$.

    %\item $\partialDeriv{\apply{\lossFunc}{\apply{\hyp_{\params}}{\example_i}, \knownLabel_i}}{o_k^{L}}$: last layer, $\lossFunc$ must be differentiable w.r.t $o_k^{L}$

      \item $\partialDeriv{o_k^{\ell+1}}{s_k^{\ell+1}}$: $o_k^{\ell+1} = \activFunc(s_k^{\ell+1})$, $\activFunc$ as intermediate or final activation function must be differentiable.

      \item $\partialDeriv{s_k^{\ell+1}}{w_{k,p}^{\ell}} = \partialDeriv{\partial}{w_{k,p}^{\ell}} \sum_{p' \in \left\{1,\ldots,N_l \right\}}  w_{p', k}^{\ell}o_{p'}^{k}= \partialDeriv{}{w_{k,p}^{\ell}}  w_{p, k}^{\ell}o_{p}^{\ell}=o_{p}^{\ell}$

    \end{itemize}
    \end{block}
  \end{textblock}

  \begin{textblock}{90}(5, 65)
    \begin{block}{General idea}<2->
      \begin{itemize}
      \item Back-propagation works for all feed-forward architectures
        \hyperlink{MLP_Learning_2}{\beamergotobutton{Go back}}
        % Back-propagates to main slide :)
      \end{itemize}
    \end{block}
  \end{textblock}
\end{frame}
