% Appendices
% To be used with hyper links

\begin{frame}[label=Backpropagation]
  \frametitle{\acl{MLP}: Compute the gradient (1/2)}

  \begin{textblock}{90}(5, 15)
    \begin{itemize}
    \item Gradient:
      $
      \grad_{\params} \apply{\lossFunc}{\apply{\hyp_{\params}}{\example_i}, \knownLabel_i} =
      \left( \partialDeriv{\apply{\lossFunc}{\apply{\hyp_{\params}}{\example_i},
            \knownLabel_i}}{w_{1,1}^1}, \ldots, \partialDeriv{\apply{\lossFunc}{\apply{\hyp_{\params}}{\example_i}, \knownLabel_i}}{w_{k,p}^{\layer}},\ldots,\partialDeriv{\apply{\lossFunc}{\apply{\hyp_{\params}}{\example_i}, \knownLabel_i}}{w_{N_{4},N_{3}}^{3}} \right)^T
      $

      \begin{itemize}
      \item Analytical calculation is possible but tedious
      \end{itemize}
    \item $\hyp_{\params}$ is a composition of functions:
      \[
        \apply{\hyp_{\params}}{\example} = \apply{\softMaxFunc}{%
          \weightMatrix^3 \times
          \apply{\ReLUFunc}{%
            \weightMatrix^2 \times
            \apply{\ReLUFunc}{%
              \weightMatrix^1 \times \example
            }
          }
        }
      \]
      \begin{itemize}
      \item Use the chain rule to compute partial derivatives.
      \end{itemize}
    \end{itemize}
  \end{textblock}
\end{frame}

\begin{frame}
  \frametitle{\acl{MLP}: Compute the gradient (2/2)}

  \begin{textblock}{90}(5, 15)
    \begin{block}{Derivation of the gradient}
      \begin{itemize}
      \item Apply the chain rule at layer $\layer$:
        $
        \partialDeriv{\apply{\lossFunc}{\apply{\hyp_{\params}}{\example_i}, \knownLabel_i}}{w_{k,p}^\layer} =
        \partialDeriv{\apply{\lossFunc}{\apply{\hyp_{\params}}{\example_i}, \knownLabel_i}}{o_k^{\layer+1}} \partialDeriv{o_k^{\layer+1}}{s_k^{\layer+1}} \partialDeriv{s_k^{\layer+1}}{w_{k,p}^{\layer}}
        $

      \item $\partialDeriv{\apply{\lossFunc}{\apply{\hyp_{\params}}{\example_i}, \knownLabel_i}}{o_k^{\layer+1}}$: known at the $\layer+1$ layer. $\partialDeriv{\apply{\lossFunc}{\apply{\hyp_{\params}}{\example_i}, \knownLabel_i}}{o_k^{L}}$: last layer, $\lossFunc$ must be differentiable w.r.t $o_k^{L}$.

    %\item $\partialDeriv{\apply{\lossFunc}{\apply{\hyp_{\params}}{\example_i}, \knownLabel_i}}{o_k^{L}}$: last layer, $\lossFunc$ must be differentiable w.r.t $o_k^{L}$

      \item $\partialDeriv{o_k^{\layer+1}}{s_k^{\layer+1}}$: $o_k^{\layer+1} = \activFunc(s_k^{\layer+1})$, $\activFunc$ as intermediate or final activation function must be differentiable.

      \item $\partialDeriv{s_k^{\layer+1}}{w_{k,p}^{\layer}} = \partialDeriv{}{w_{k,p}^{\layer}} \sum_{p' \in \left\{1,\ldots,N_l \right\}}  w_{p', k}^{\layer}o_{p'}^{k}= \partialDeriv{}{w_{k,p}^{\layer}}  w_{p, k}^{\layer}o_{p}^{\layer}=o_{p}^{\layer}$

    \end{itemize}
    \end{block}
  \end{textblock}

  \begin{textblock}{90}(5, 65)
    \begin{block}{General idea}<2->
      \begin{itemize}
      \item Back-propagation works for all feed-forward architectures
        \hyperlink{MLP_Learning_2}{\beamergotobutton{Go back}}
        % Back-propagates to main slide :)
      \end{itemize}
    \end{block}
  \end{textblock}
\end{frame}
